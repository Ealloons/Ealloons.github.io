---
layout: single
title: "#1 Raspberry Pi 와 Kafka 통신하기 feat. PyQt5"
---
# 개요
요새 스마트팜이 떠오르는 추세인것 같더라구요 <br>
그래서 라즈베리파이의 센서데이터를 받아 GUI로 확인할 수 있는 <br>
프로그램을 만들고자 합니다
프로그램의 완성 코드는 [여기](https://github.com/Ealloons/IoT_RaspberryPi4_with_Kafka) 에 있습니다 <br>
이 포스트는 코드 해석 중점이므로 코드와 함께 읽으시면 이해가 빠르실 겁니다. <br>

# Kafka
Kafka에 대한 튜토리얼은 많이 있기 때문에 설치 과정은 생략하겠습니다 <br>
제가 임시로 생성해 놓은 Topic은 temp_test 와 imgtest 입니다. 
## Producer - 라즈베리 파이
먼저  데이터를 송신하기 위해 라즈베리 파이가 Producer 역할을 해줘야 합니다. <br>
### test.py
```python
import time
from datetie import datetime
import board
import adafruit_dht
from json import dumps
from kafka import KafkaProducer
producer = KafkaProducer(acks=0, compression_type='gzip', bootstrap_servers=['192.168.35.145:9092'],
                         value_serializer=lambda x: dumps(x).encode('utf-8'))
#Initial the dht device, with data pin connected to:
dhtDevice = adafruit_dht.DHT11(board.D4)
while True:
    try:
         # Print the values to the serial port
         temperature_c = dhtDevice.temperature
         temperature_f = temperature_c * (9 / 5) + 32
         humidity = dhtDevice.humidity
         print("Temp: {:.1f} F / {:.1f} C    Humidity: {}% "
               .format(temperature_f, temperature_c, humidity))
         now = datetime.now()
         data = {'m_id' : '0',
                 'datetime' : now.strftime("%Y-%m-%d %H:%M:%S"),
                 'temp' : str(temperature_c)}
         producer.send('temp_test', value=data)
         producer.flush()
    except RuntimeError as error:     # Errors happen fairly often, DHT's are hard to read, just keep going
         print(error.args[0])
    time.sleep(8.0)
```
먼저 상단에 KafkaProducer로 producer 객체를 생성합니다. <br>
코드에 따라  temp와 datetime 등의 데이터를 python의 dict로 래핑해줍니다. <br>
 이 값을 temp_test라는 토픽으로 보내는데, 위의 producer를 생성할때 value_serializer 파라미터에 넣은 함수를 통해 json 형식으로 넘어가게 됩니다.
#### camera_test.py
```python
import cv2
import picamera
import time
from kafka import KafkaProducer
from json import dumps
from datetime import datetime
import base64
import json
import numpy as np

producer = KafkaProducer(acks=0, compression_type='gzip', bootstrap_servers=['192.168.35.145:9092'],
                         value_serializer=lambda x: dumps(x).encode('utf-8'))
camera = picamera.PiCamera()

while True:
    time.sleep(100)
    now = datetime.now()
    now = now.strftime("%Y-%m-%d %H:%M:%S")
    camera.annotate_text = now
    camera.resolution=(1024,768)
    camera.capture('snapshot.jpg',resize=(500,500))
    img = cv2.imread('./snapshot.jpg')
    image_string = cv2.imencode('.jpg', img)[1]
    image_string = base64.b64encode(image_string).decode()
    print('send image')
    image = {
    'm_id' : '0',
    'datetime' : now,
    'photo': image_string
    }
    producer.send('imgtest', value=image)
    producer.flush()
```
위에서 말씀드렸듯이 producer는 json 타입으로 데이터를 보냅니다. <br>
json 데이터는 모든 요소가 str이어야 하므로 이미지를 바이너리 데이터로 <br>
바꾼 후 str으로 바꿔서 보내야 합니다. 이를 위해서 이미지를 opencv로 encode 한 다음 base64로 한번더 encode한 다음 decode하여 바이너리 데이터를 안전하게 <br> str 타입으로 바꿀 수 있습니다.
## Consumer 
이제 Kafka에 접속하여 데이터를 수신할 Consumer를 구현하겠습니다. <br>
저는 수신된 데이터를 Pymongo 에 저장하도록 코딩했습니다. <br>
```python
from pymongo import MongoClient
from kafka import KafkaConsumer
from json import loads
import os
from confluent_kafka import Consumer
from datetime import datetime
client = MongoClient("mongodb://localhost:27017/")
db = client['test-db']
c = Consumer({
    'bootstrap.servers' : '192.168.35.145',
    'group.id' : 'my-group',
    'auto.offset.reset' : 'earliest',
})
c.subscribe(['temp_test','imgtest'])
while True:
    msg = c.poll()
    if msg.error():
        print("consumer error: {}".format(msg.error()))
        continue
    if msg.topic() == "temp_test":
        collection = 'temperature'
    elif msg.topic() == "imgtest":
        print("done!")
        collection = 'image-test'
    else:
        collection = "no"
    data = loads(msg.value().decode('utf-8'))
    data['datetime'] = datetime.strptime(data['datetime'], '%Y-%m-%d %H:%M:%S')
    print(data)
    db[collection].insert_one(data)
```
저는 임시로 test-db를 만들었습니다. 추가로 여기서는 여러 토픽에 대해 데이터를 <br>
수신할 것이기 때문에 confluent_kafka 모듈로 Consumer를 구현했습니다. <br>
subscribe 는 수신받을 토픽의 리스트입니다. 반복문에는 어떤 topic이냐에 따라 <br>
저장할 DB를 지정해줬습니다. 저것 보다 많아지면 따로 함수를 만드는게 좋을 것 같습니다<br>
이 코드의 핵심은 data의 datetime 인덱스를 바꾼다는 것입니다. <br>
msg의 value는 바이너리 값이고 decode하면 json형태의 str입니다. <br>
loads를 통해 dict로 바꿔야 각 인덱스에 접근할 수 있는데, consumer에서 datetime을 str로 보냈기 때문에 datetime을 python의 datetime 객체로 바꾼 다음 <br>
DB에 넣어야 합니다. 이는 나중에 쿼리를 보냈을때 날짜간의 크기를 비교하기 위해서 필요합니다. <br> str 그대로 넣으면 쿼리가 제대로 실행되지 않습니다.
